




















import jieba
import os
import wordcloud
import matplotlib.pyplot as plt
from imageio import imread
#得到文本
def getText(filepath):
    with open(filepath,"r",encoding = 'utf-8') as f:
        text = f.read()
        return text

#增加停用词
def stopwordslist(filepath):
    stopwords = [line.strip() for line in  open(filepath, 'r',encoding='utf-8').readlines()]
    return stopwords

#分频统计词频
def wordFreq(filepath,text,topn):
    #使用jieba库中的lcut对文本文件分词
    words = jieba.lcut(text.strip())
    counts = {}
    stopwords = stopwordslist('stop_words.txt')
    #删除长度为1的字符
    for word in words:
        if len(word) == 1:
            continue
        elif word not in stopwords :
    # 人物名称统一处理
            if word == '云长'or word == '关公': word = '关羽'
            elif word == '玄德': word = '刘备'
            elif word == '孟德': word = '曹操'
            elif word == '翼德': word = '张飞' 
            elif word == '子龙': word = '赵云'
            elif word == '奉先': word = '吕布'
            elif word == '仲达': word = '司马懿'
            elif word == '公瑾': word = '周瑜'
            elif word == '伯符': word = '孙策'
            elif word == '仲谋': word = '孙权'
            elif word == '文远': word = '张辽'
            elif word == '妙才': word = '夏侯渊'
            elif word == '元让': word = '夏侯惇'
            elif word == '子和': word = '曹纯'
            elif word == '文若': word = '荀彧'
            elif word == '奉孝': word = '郭嘉'
            elif word == '子孝': word = '曹仁'
            elif word == '文和': word = '贾诩'
            elif word == '汉升': word = '黄忠'
            elif word == '幼平': word = '周泰'
            elif word == '子明': word = '吕蒙'
            elif word == '子敬': word = '鲁肃'
            elif word == '兴霸': word = '甘宁'
            elif word == '元直': word = '徐庶'
            elif word == '士元': word = '庞统'
            elif word == '孔明': word = '诸葛亮'
            elif word == '季常': word = '马良'
            elif word == '文长': word = '魏延'
            elif word == '孝直': word = '法正'
            elif word == '子衡': word = '步骘'
            elif word == '子布': word = '张昭'
            # 女性角色
            elif word == '尚香': word = '孙夫人'
            elif word == '蝉儿': word = '貂蝉'
            # 君主尊称
            elif word == '吴侯': word = '孙权'
            elif word == '晋王': word = '司马昭'
            # 敌我双方称呼
            elif word == '诸葛村夫': word = '诸葛亮'
            elif word == '碧眼儿': word = '孙权'
            # 父子兄弟
            elif word == '伯约': word = '姜维'
            elif word == '思远': word = '张悌'
            # 特殊称谓
            elif word == '卧龙': word = '诸葛亮'
            elif word == '凤雏': word = '庞统'
            elif word == '虎痴': word = '许褚'
            elif word == '冢虎': word = '司马懿'
            # 异族首领
            elif word == '单于': word = '匈奴王'
            elif word == '大汗': word = '轲比能'
            # 补充遗漏
            elif word == '子远': word = '许攸'
            elif word == '公台': word = '陈宫'
            elif word == '元皓': word = '田丰'
            elif word == '异度': word = '蒯越'
            elif word == '子将': word = '许劭'
            # 处理复姓
            elif word in ['孔明', '诸葛']: word = '诸葛亮'
            elif word in ['仲达', '司马']: word = '司马懿'
            
            counts[word] = counts.get(word,0) + 1
    items = list(counts.items())
    items.sort(key = lambda x:x[1], reverse = True )
    with open(filepath[:-4]+'_词频.txt',"w",encoding = 'utf-8') as f:
        for i in range(topn):
            word,count = items[i]
            f.write("{}\t{}\n".format(word,count))
    return f,items[:topn]

def create_wordcloud(freq_file, font_path='simhei.ttf', mask_file=None):
    # 读取词频文件
    with open(freq_file, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # 将词频转换为字典
    freq_dict = {}
    for line in text.split('\n'):
        if '\t' in line:
            word, count = line.split('\t')
            freq_dict[word] = int(count)
    
    # 设置词云参数
    wc = wordcloud.WordCloud(
        font_path=font_path,    # 中文字体（需要下载simhei.ttf）
        width=1200,             # 宽度
        height=800,             # 高度
        background_color='white',# 背景色
        max_words=200,          # 最大显示词数
        max_font_size=150,      # 最大字体尺寸
        scale=2,                # 清晰度倍增系数
        collocations=False,     # 避免重复词汇
        prefer_horizontal=0.9  # 水平显示比例
    )
    
    # 如果有蒙版图片
    if mask_file and os.path.exists(mask_file):
        mask = imread(mask_file)
        wc.mask = mask
    
    # 生成词云
    wc.generate_from_frequencies(freq_dict)
    
    # 显示词云
    plt.figure(dpi=120)  # 设置显示分辨率
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.show()
    
    # 保存图片
    wc.to_file(freq_file[:-4]+'_词云.png')
    print(f"词云已保存至：{freq_file[:-4]}_词云.png")

def main():
    os.chdir(r"D:\studycode\python\pytest\test7")
    filepath = "《三国演义》.txt"
    text = getText(filepath)
    f1, items = wordFreq(filepath, text, 10)
    
    # 生成词云（需要simhei.ttf字体文件）
    create_wordcloud(
        freq_file=filepath[:-4]+'_词频.txt',
        font_path='simhei.ttf',
        mask_file='china_map.png'  # 可选蒙版图片
    )
    
    # 打印前10结果
    for name,freq in items:
        print("{}\t{}".format(name,freq))
if __name__ == "__main__":
    main()







'上机测试jieba库中的分词函数，并尝试使用用户自定义词典'
import jieba
import re
import importlib
import os

def clean_jieba_cache():
    """彻底清除jieba缓存和初始化状态"""
    jieba.dt.cache = None
    jieba.dt.initialized = False
    jieba.dt.tmp_dir = None
    # 重置正则表达式为原始状态
    jieba.re_han_default = re.compile(r'([\u4E00-\u9FD5]+)', re.UNICODE)

def test_jieba_custom_dict():
    # 测试文本
    text = "我爱自然语言处理"
    
    try:
        # 清理环境（重要！）
        clean_jieba_cache()
        
        # ---------------------------
        # 场景1：原始默认分词
        # ---------------------------
        default_seg = jieba.cut(text)
        print("[场景1] 默认分词结果:", "/".join(default_seg))
        
        # ---------------------------
        # 场景2：使用文件词典
        # ---------------------------
        # 创建临时词典文件
        dict_file = "userdict.txt"
        with open(dict_file, "w", encoding="utf-8") as f:
            f.write("自然语言处理 10000 n\n")
        
        # 加载词典前再次清理环境
        clean_jieba_cache()
        jieba.load_userdict(dict_file)
        
        file_dict_seg = jieba.cut(text)
        print("[场景2] 文件词典分词:", "/".join(file_dict_seg))
        
        # ---------------------------
        # 场景3：动态添加词语
        # ---------------------------
        # 清理环境并重新初始化
        clean_jieba_cache()
        jieba.initialize()
        
        jieba.add_word("自然语言处理", freq=10000, tag='n')
        dynamic_seg = jieba.cut(text)
        print("[场景3] 动态添加分词:", "/".join(dynamic_seg))
        
        # ---------------------------
        # 场景4：调整词频强制合并
        # ---------------------------
        clean_jieba_cache()
        jieba.initialize()
        
        # 修正正则表达式（使用原始字符串）
        jieba.re_han_default = re.compile(
            r'([\u4E00-\u9FD5a-zA-Z0-9+#&._]+)',  # 注意这里的 . 不需要转义
            re.UNICODE
        )
        jieba.suggest_freq("自然语言处理", tune=True)
        
        freq_seg = jieba.cut(text)
        print("[场景4] 词频调整分词:", "/".join(freq_seg))
        
    finally:
        # 清理临时文件
        if os.path.exists(dict_file):
            os.remove(dict_file)

if __name__ == "__main__":
    test_jieba_custom_dict()





from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re

def generate_wordcloud(text):
    stopwords = {'the','and','of','a','in','to','is','it','that','this','be','are'}
    cleaned = re.sub(r'[^a-zA-Z]', ' ', text.lower())
    words = [word for word in cleaned.split() if word not in stopwords and len(word)>2]
    wc = WordCloud(width=800, height=600, background_color='white', max_words=50).generate(' '.join(words))
    plt.imshow(wc)
    plt.axis('off')
    plt.show()

sample_text = """
Artificial intelligence (AI) is transforming every industry. Machine learning algorithms 
enable computers to learn from data without explicit programming. Deep learning uses neural 
networks with multiple layers to analyze complex patterns. Natural language processing (NLP) 
helps computers understand human language. AI applications range from chatbots to self-driving cars.
"""
generate_wordcloud(sample_text)









